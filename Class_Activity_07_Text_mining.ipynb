{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wwiZq8WdC8QC"
   },
   "source": [
    "# Chapter 20: Text mining\n",
    "\n",
    "> (c) 2019 Galit Shmueli, Peter C. Bruce, Peter Gedeck\n",
    ">\n",
    "> Code included in\n",
    ">\n",
    "> _Data Mining for Business Analytics: Concepts, Techniques, and Applications in Python_ (First Edition)\n",
    "> Galit Shmueli, Peter C. Bruce, Peter Gedeck, and Nitin R. Patel. 2019.\n",
    "\n",
    "## Import required packages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NmfY5utnC8QD"
   },
   "source": [
    "Make sure DMBA package is available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "pbuao-8fC8QD",
    "outputId": "ca52dcd3-41f0-488b-ce1d-b857463d5708"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting nltk\n",
      "  Downloading nltk-3.9.1-py3-none-any.whl.metadata (2.9 kB)\n",
      "Collecting click (from nltk)\n",
      "  Downloading click-8.1.7-py3-none-any.whl.metadata (3.0 kB)\n",
      "Requirement already satisfied: joblib in /home/alago/.local/share/pipx/venvs/jupyterlab/lib/python3.12/site-packages (from nltk) (1.4.2)\n",
      "Collecting regex>=2021.8.3 (from nltk)\n",
      "  Downloading regex-2024.9.11-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (40 kB)\n",
      "Collecting tqdm (from nltk)\n",
      "  Downloading tqdm-4.66.6-py3-none-any.whl.metadata (57 kB)\n",
      "Downloading nltk-3.9.1-py3-none-any.whl (1.5 MB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading regex-2024.9.11-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (797 kB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m797.0/797.0 kB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading click-8.1.7-py3-none-any.whl (97 kB)\n",
      "Downloading tqdm-4.66.6-py3-none-any.whl (78 kB)\n",
      "Installing collected packages: tqdm, regex, click, nltk\n",
      "Successfully installed click-8.1.7 nltk-3.9.1 regex-2024.9.11 tqdm-4.66.6\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49m/home/alago/.local/share/pipx/venvs/jupyterlab/bin/python -m pip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZAbcIrGSHHy0"
   },
   "source": [
    "\n",
    "\n",
    "\n",
    "- **Library Imports**: Each library or module is imported for a specific purpose, mostly related to text processing, machine learning, and data visualization.\n",
    "- **NLTK Download**: Downloads the Punkt tokenizer model for splitting text into individual words. This is essential for the tokenization process in NLTK.\n",
    "- **Matplotlib Inline**: `%matplotlib inline` is specific to Jupyter Notebooks, allowing plots to be shown inline. This line is not necessary if running outside of Jupyter.\n",
    "\n",
    "This setup prepares you for a text mining pipeline that includes:\n",
    "1. Reading and processing text data.\n",
    "2. Transforming text data into a numeric form.\n",
    "3. Reducing the dimensionality of the data using LSA.\n",
    "4. Fitting a logistic regression model to predict labels based on text.\n",
    "5. Using custom functions for data summaries and visualization.\n",
    "\n",
    "Each step in this code sets up necessary libraries and tools to perform these actions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "5SnCAqH_C8QE",
    "outputId": "86d85194-d6cc-41ba-f5a5-bbe6084bd69b"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/alago/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to /home/alago/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
     ]
    }
   ],
   "source": [
    "# Importing necessary libraries for file handling, data processing, machine learning, and text processing\n",
    "from pathlib import Path  # To handle file paths\n",
    "from zipfile import ZipFile  # To work with zip files\n",
    "import pandas as pd  # Data manipulation library\n",
    "from sklearn.model_selection import train_test_split  # For splitting data into training and test sets\n",
    "from sklearn.feature_extraction.text import CountVectorizer  # For converting text to a bag-of-words model\n",
    "from sklearn.feature_extraction.text import TfidfTransformer  # To apply TF-IDF transformation on bag-of-words model\n",
    "from sklearn.decomposition import TruncatedSVD  # For dimensionality reduction, used here for LSA\n",
    "from sklearn.preprocessing import Normalizer  # To normalize the data after SVD\n",
    "from sklearn.pipeline import make_pipeline  # To create a pipeline that applies multiple transformations in sequence\n",
    "from sklearn.linear_model import LogisticRegression  # Logistic regression model for classification\n",
    "import nltk  # Natural Language Toolkit for text processing\n",
    "from nltk import word_tokenize  # For tokenizing text data into words\n",
    "from nltk.stem.snowball import EnglishStemmer  # For stemming English words to their root forms\n",
    "import matplotlib.pylab as plt  # For data visualization\n",
    "\n",
    "# Importing additional functions from the dmba library, possibly custom to the user environment\n",
    "import dmba\n",
    "from dmba import printTermDocumentMatrix, classificationSummary, liftChart  # Specific dmba functions\n",
    "\n",
    "# Downloading the Punkt tokenizer models, necessary for word tokenization in NLTK\n",
    "nltk.download('punkt')\n",
    "nltk.download('punkt_tab')\n",
    "\n",
    "# Setting up inline plotting for Jupyter Notebook; displays plots directly below code cells\n",
    "%matplotlib inline\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "j_RYmGnJC8QE"
   },
   "source": [
    "## Term-document representation of words in sentences S1-S3\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KiPPzKbpL2gB"
   },
   "source": [
    "**Question 1:**\n",
    "\n",
    "You are given the following three sentences, and your task is to construct a term-document matrix based on these sentences using the `CountVectorizer` from `scikit-learn`. The matrix should display each unique word across the sentences, with each cell showing the count of the word’s occurrence in that sentence.\n",
    "\n",
    "Sentences:\n",
    "1. \"this is the first sentence.\"\n",
    "2. \"this is a second sentence.\"\n",
    "3. \"the third sentence is here.\"\n",
    "\n",
    "**Steps:**\n",
    "1. Identify all unique words across the sentences.\n",
    "2. For each sentence, count how often each unique word appears.\n",
    "3. Construct the term-document matrix, where rows represent words and columns represent sentences (S1, S2, S3).\n",
    "\n",
    "**Hints:**\n",
    "- Use `CountVectorizer` to automatically tokenize and count each word.\n",
    "- `printTermDocumentMatrix` is a helpful function that can format your matrix for easy viewing.\n",
    "- Remember to remove any punctuation and convert words to lowercase for consistency.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "ocvquguvC8QE",
    "outputId": "16b9b163-7a25-4c46-a381-b26154c33b83"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                S1        S2        S3\n",
      "first     2.098612  0.000000  0.000000\n",
      "here      0.000000  0.000000  2.098612\n",
      "is        1.000000  1.000000  1.000000\n",
      "second    0.000000  2.098612  0.000000\n",
      "sentence  1.000000  1.000000  1.000000\n",
      "the       1.405465  0.000000  1.405465\n",
      "third     0.000000  0.000000  2.098612\n",
      "this      1.405465  1.405465  0.000000\n"
     ]
    }
   ],
   "source": [
    "# Sample text data: three sentences to be represented in a term-document matrix\n",
    "text = [\n",
    "    'this is the first sentence.',\n",
    "    'this is a second sentence.',\n",
    "    'the third sentence is here.'\n",
    "]\n",
    "\n",
    "# write code here\n",
    "count_vect = CountVectorizer()\n",
    "tfidTransformer = TfidfTransformer(smooth_idf=False, norm=None)\n",
    "counts = count_vect.fit_transform(text)\n",
    "tfid = tfidTransformer.fit_transform(counts)\n",
    "\n",
    "printTermDocumentMatrix(count_vect, tfid)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LTlRB3zKC8QE"
   },
   "source": [
    "**Question 2:**\n",
    "\n",
    "You are given four sentences with mixed casing and special characters. Your task is to construct a term-document matrix based on these sentences using `CountVectorizer` in Python. The matrix should display each unique word across the sentences, with each cell showing the count of that word’s occurrence in each sentence.\n",
    "\n",
    "Sentences:\n",
    "1. \"this is the first sentence!!\"\n",
    "2. \"this is a second Sentence :)\"\n",
    "3. \"the third sentence, is here\"\n",
    "4. \"forth of all sentences\"\n",
    "\n",
    "**Steps:**\n",
    "1. **Identify the unique words** that will appear as columns in the matrix.\n",
    "2. **Construct the term-document matrix**, ensuring that punctuation is ignored and text is treated as lowercase.\n",
    "3. **Analyze the output** to check if any special characters or mixed cases affected the word counting.\n",
    "\n",
    "**Hints:**\n",
    "- By default, `CountVectorizer` will convert all words to lowercase and remove punctuation.\n",
    "- Each row in the output should represent a unique word, while each column represents a sentence (S1, S2, S3, S4).\n",
    "- Use `printTermDocumentMatrix` to view the matrix in an organized format.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "pu7SWegvC8QE",
    "outputId": "3bd97dac-80b9-47b5-e079-6bb52139f67e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 S1        S2        S3        S4\n",
      "all        0.000000  0.000000  0.000000  2.386294\n",
      "first      2.386294  0.000000  0.000000  0.000000\n",
      "forth      0.000000  0.000000  0.000000  2.386294\n",
      "here       0.000000  0.000000  2.386294  0.000000\n",
      "is         1.287682  1.287682  1.287682  0.000000\n",
      "of         0.000000  0.000000  0.000000  2.386294\n",
      "second     0.000000  2.386294  0.000000  0.000000\n",
      "sentence   1.287682  1.287682  1.287682  0.000000\n",
      "sentences  0.000000  0.000000  0.000000  2.386294\n",
      "the        1.693147  0.000000  1.693147  0.000000\n",
      "third      0.000000  0.000000  2.386294  0.000000\n",
      "this       1.693147  1.693147  0.000000  0.000000\n"
     ]
    }
   ],
   "source": [
    "# Sample text data: Four sentences with special characters and mixed casing\n",
    "text = [\n",
    "    'this is the first sentence!!',         # S1\n",
    "    'this is a second Sentence :)',         # S2\n",
    "    'the third sentence, is here ',         # S3\n",
    "    'forth of all sentences'                # S4\n",
    "]\n",
    "\n",
    "\n",
    "# write code here\n",
    "\n",
    "count_vect = CountVectorizer()\n",
    "tfidTransformer = TfidfTransformer(smooth_idf=False, norm=None)\n",
    "counts = count_vect.fit_transform(text)\n",
    "tfid = tfidTransformer.fit_transform(counts)\n",
    "\n",
    "printTermDocumentMatrix(count_vect, tfid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "i7cN3ZzAC8QF"
   },
   "source": [
    "\n",
    "### **Question 3:**\n",
    "\n",
    "You are given a set of four sentences with special characters and varied formatting. Your task is to create a term-document matrix using `CountVectorizer` with a custom token pattern that treats special characters as part of the tokens.\n",
    "\n",
    "Sentences:\n",
    "1. \"This is the first     sentence!!\"\n",
    "2. \"this is a second Sentence :)\"\n",
    "3. \"the third sentence, is here \"\n",
    "4. \"forth of all sentences\"\n",
    "\n",
    "**Steps:**\n",
    "1. **Identify Tokens**: Review the token pattern `[a-zA-Z!:)]+` and determine which words and symbols will be considered as tokens.\n",
    "2. **Generate the Term-Document Matrix**: Use `CountVectorizer` with the custom token pattern to build the term-document matrix.\n",
    "3. **Analyze the Output**: Interpret how different words and special characters are counted in each sentence.\n",
    "\n",
    "**Hints:**\n",
    "- The token pattern `[a-zA-Z!:)]+` includes alphabetical characters, exclamation marks (`!`), colons (`:`), and parentheses (`)`).\n",
    "- Each token is treated as a single entity, even if it contains special characters.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "mwX4RbG9C8QF",
    "outputId": "1cf2ce6f-6f44-4b25-c59c-081a53abac02"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            S1  S2  S3  S4\n",
      ":)           0   1   0   0\n",
      "a            0   1   0   0\n",
      "all          0   0   0   1\n",
      "first        1   0   0   0\n",
      "forth        0   0   0   1\n",
      "here         0   0   1   0\n",
      "is           1   1   1   0\n",
      "of           0   0   0   1\n",
      "second       0   1   0   0\n",
      "sentence     0   1   1   0\n",
      "sentence!!   1   0   0   0\n",
      "sentences    0   0   0   1\n",
      "the          1   0   1   0\n",
      "third        0   0   1   0\n",
      "this         1   1   0   0\n"
     ]
    }
   ],
   "source": [
    "# Sample text data with special characters and varied formatting\n",
    "text = [\n",
    "    'this is the first     sentence!!',      # S1\n",
    "    'this is a second Sentence :)',          # S2\n",
    "    'the third sentence, is here ',          # S3\n",
    "    'forth of all sentences'                 # S4\n",
    "]\n",
    "\n",
    "# write code here\n",
    "\n",
    "count_vect = CountVectorizer(token_pattern='[a-zA-Z!:)]+')\n",
    "counts = count_vect.fit_transform(text)\n",
    "\n",
    "printTermDocumentMatrix(count_vect, counts)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eGD2MOL7C8QF"
   },
   "source": [
    "### **Question 4: Exploring Stopwords in Text Processing**\n",
    "\n",
    "**Objective:** Understand the role of stopwords in text preprocessing and how they can impact the analysis of textual data.\n",
    "\n",
    "**Question:**\n",
    "\n",
    "1. **Reviewing Stopwords**: Examine the list of English stopwords displayed in the output above.\n",
    "   - How many stopwords are included in scikit-learn’s default set?\n",
    "   - Why might certain words, like \"above,\" \"below,\" and \"upon,\" be considered stopwords?\n",
    "\n",
    "2. **Impact on Text Analysis**:\n",
    "   - Discuss how removing these stopwords could change the outcome of a text analysis or classification task. Provide examples of when removing stopwords might be beneficial and when it might not be helpful.\n",
    "   \n",
    "3. **Customization**:\n",
    "   - Imagine you are analyzing a dataset where words like \"data\" and \"science\" appear frequently but add little value to your analysis.\n",
    "     - How would you modify the stopword list in scikit-learn to include additional words that are specific to your task?\n",
    "\n",
    "**Hint:**\n",
    "- Stopwords are commonly used words that do not carry specific meaning in analysis and are often removed to reduce noise in data. Think about examples in real text data where removing words might help highlight important terms.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "WwzMUAaqC8QF",
    "outputId": "b800fc53-b468-40d1-d1d3-07b708f20194"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['a', 'about', 'above', 'across', 'after', 'afterwards', 'again', 'against', 'all', 'almost', 'alone', 'along', 'already', 'also', 'although', 'always', 'am', 'among', 'amongst', 'amoungst', 'amount', 'an', 'and', 'another', 'any', 'anyhow', 'anyone', 'anything', 'anyway', 'anywhere', 'are', 'around', 'as', 'at', 'back', 'be', 'became', 'because', 'become', 'becomes', 'becoming', 'been', 'before', 'beforehand', 'behind', 'being', 'below', 'beside', 'besides', 'between', 'beyond', 'bill', 'both', 'bottom', 'but', 'by', 'call', 'can', 'cannot', 'cant', 'co', 'con', 'could', 'couldnt', 'cry', 'de', 'describe', 'detail', 'do', 'done', 'down', 'due', 'during', 'each', 'eg', 'eight', 'either', 'eleven', 'else', 'elsewhere', 'empty', 'enough', 'etc', 'even', 'ever', 'every', 'everyone', 'everything', 'everywhere', 'except', 'few', 'fifteen', 'fifty', 'fill', 'find', 'fire', 'first', 'five', 'for', 'former', 'formerly', 'forty', 'found', 'four', 'from', 'front', 'full', 'further', 'get', 'give', 'go', 'had', 'has', 'hasnt', 'have', 'he', 'hence', 'her', 'here', 'hereafter', 'hereby', 'herein', 'hereupon', 'hers', 'herself', 'him', 'himself', 'his', 'how', 'however', 'hundred', 'i', 'ie', 'if', 'in', 'inc', 'indeed', 'interest', 'into', 'is', 'it', 'its', 'itself', 'keep', 'last', 'latter', 'latterly', 'least', 'less', 'ltd', 'made', 'many', 'may', 'me', 'meanwhile', 'might', 'mill', 'mine', 'more', 'moreover', 'most', 'mostly', 'move', 'much', 'must', 'my', 'myself', 'name', 'namely', 'neither', 'never', 'nevertheless', 'next', 'nine', 'no', 'nobody', 'none', 'noone', 'nor', 'not', 'nothing', 'now', 'nowhere', 'of', 'off', 'often', 'on', 'once', 'one', 'only', 'onto', 'or', 'other', 'others', 'otherwise', 'our', 'ours', 'ourselves', 'out', 'over', 'own', 'part', 'per', 'perhaps', 'please', 'put', 'rather', 're', 'same', 'see', 'seem', 'seemed', 'seeming', 'seems', 'serious', 'several', 'she', 'should', 'show', 'side', 'since', 'sincere', 'six', 'sixty', 'so', 'some', 'somehow', 'someone', 'something', 'sometime', 'sometimes', 'somewhere', 'still', 'such', 'system', 'take', 'ten', 'than', 'that', 'the', 'their', 'them', 'themselves', 'then', 'thence', 'there', 'thereafter', 'thereby', 'therefore', 'therein', 'thereupon', 'these', 'they', 'thick', 'thin', 'third', 'this', 'those', 'though', 'three', 'through', 'throughout', 'thru', 'thus', 'to', 'together', 'too', 'top', 'toward', 'towards', 'twelve', 'twenty', 'two', 'un', 'under', 'until', 'up', 'upon', 'us', 'very', 'via', 'was', 'we', 'well', 'were', 'what', 'whatever', 'when', 'whence', 'whenever', 'where', 'whereafter', 'whereas', 'whereby', 'wherein', 'whereupon', 'wherever', 'whether', 'which', 'while', 'whither', 'who', 'whoever', 'whole', 'whom', 'whose', 'why', 'will', 'with', 'within', 'without', 'would', 'yet', 'you', 'your', 'yours', 'yourself', 'yourselves']\n"
     ]
    }
   ],
   "source": [
    "# Importing the default list of English stopwords from scikit-learn\n",
    "from sklearn.feature_extraction._stop_words import ENGLISH_STOP_WORDS\n",
    "\n",
    "# write code here\n",
    "stopWords = list(sorted(ENGLISH_STOP_WORDS))\n",
    "print(stopWords)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6LvmGWTtC8QF"
   },
   "source": [
    "### **Question 5: Exploring Text Reduction Using Stemming and Stopwords**\n",
    "\n",
    "**Objective:** Understand the process and effect of stemming and stopword removal in text preprocessing.\n",
    "\n",
    "**Questions:**\n",
    "\n",
    "1. **Stemming and Stopword Removal**:\n",
    "   - Explain what the terms \"stemming\" and \"stopword removal\" mean in the context of text preprocessing. Why are these steps commonly used in natural language processing?\n",
    "\n",
    "2. **Analyzing the Output Matrix**:\n",
    "   - Look at the term-document matrix produced after stemming and stopword removal. Notice that words like \"first,\" \"this,\" and common words (stopwords) have been removed.\n",
    "   - How does stemming affect the words in this matrix? For example, what changes might occur to the word “sentences”?\n",
    "\n",
    "3. **Modifying the Preprocessor**:\n",
    "   - If you wanted to include specific words (such as “first” or “all”) in the analysis despite them being common, how would you modify the `LemmaTokenizer` class to remove these words from the stopword list?\n",
    "\n",
    "4. **Effects on Analysis**:\n",
    "   - Discuss how stemming and stopword removal could impact the analysis, especially when comparing this matrix to one where stopwords and stemming are not applied.\n",
    "\n",
    "**Hints**:\n",
    "- Remember that stemming reduces words to their base forms (e.g., “running” to “run”).\n",
    "- Stopwords are words that are often removed in text analysis because they don’t add unique meaning to the dataset.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "ATBljxFzC8QF",
    "outputId": "f374260f-c983-44e2-e6bb-93a75135833c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         S1  S2  S3  S4\n",
      "forth     0   0   0   1\n",
      "second    0   1   0   0\n",
      "sentenc   1   1   1   1\n"
     ]
    }
   ],
   "source": [
    "# Importing English stopwords from scikit-learn\n",
    "from sklearn.feature_extraction._stop_words import ENGLISH_STOP_WORDS\n",
    "from nltk import word_tokenize  # Importing word_tokenize for tokenizing the text\n",
    "from nltk.stem.snowball import EnglishStemmer  # Importing the English stemmer for stemming words\n",
    "\n",
    "# Sample text data with mixed casing and punctuation\n",
    "text = [\n",
    "    'this is the first     sentence!! ',   # S1\n",
    "    'this is a second Sentence :)',        # S2\n",
    "    'the third sentence, is here ',        # S3\n",
    "    'forth of all sentences'               # S4\n",
    "]\n",
    "\n",
    "# Create a custom tokenizer class that will apply stemming and remove stopwords\n",
    "# write code here\n",
    "class LemmaTokenizer(object):\n",
    "    def __init__(self):\n",
    "        self.stemmer = EnglishStemmer()\n",
    "        self.stopWords = ENGLISH_STOP_WORDS\n",
    "\n",
    "    def __call__(self, doc):\n",
    "        return [self.stemmer.stem(t) for t in word_tokenize(doc) \n",
    "                if t.isalpha() and t not in self.stopWords]\n",
    "\n",
    "count_vect = CountVectorizer(tokenizer=LemmaTokenizer())\n",
    "counts = count_vect.fit_transform(text)\n",
    "\n",
    "printTermDocumentMatrix(count_vect, counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XMYD-j9OC8QF"
   },
   "source": [
    "\n",
    "### **Question 6: Understanding TF-IDF Matrix for Text Data**\n",
    "\n",
    "**Objective:** Understand how the TF-IDF matrix represents the importance of terms within documents and apply this concept to analyze a sample dataset.\n",
    "\n",
    "**Questions:**\n",
    "\n",
    "1. **Basic Concept**:\n",
    "   - What does the TF-IDF value represent for a term in a specific document?\n",
    "   - Why might a term have a higher TF-IDF value in one document compared to another?\n",
    "\n",
    "2. **Analyzing the Matrix**:\n",
    "   - Look at the values in the TF-IDF matrix for the terms \"first,\" \"sentence,\" and \"here.\"\n",
    "   - Explain why these terms have different TF-IDF scores across sentences S1 to S4.\n",
    "   - Identify any term with the same TF-IDF score across all sentences. What might this indicate about the term’s presence across the sentences?\n",
    "\n",
    "3. **Effects of Parameter Settings**:\n",
    "   - The code specifies `smooth_idf=False` and `norm=None`. Discuss how changing these parameters might affect the TF-IDF values. (Hint: Smoothing helps handle terms that are missing in some documents, and normalization scales TF-IDF values to make comparisons easier.)\n",
    "\n",
    "4. **Real-World Application**:\n",
    "   - Consider how TF-IDF could be helpful in filtering out common but less informative terms in a large dataset. How could this matrix be used in applications like document classification or information retrieval?\n",
    "\n",
    "**Hints**:\n",
    "- Remember, a higher TF-IDF score indicates that a term is more unique to a particular document in the dataset.\n",
    "- Terms that appear in all documents tend to have lower scores, as they are less useful for distinguishing documents from each other.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-26T14:23:16.706732Z",
     "iopub.status.busy": "2023-06-26T14:23:16.706298Z",
     "iopub.status.idle": "2023-06-26T14:23:16.715982Z",
     "shell.execute_reply": "2023-06-26T14:23:16.715195Z"
    },
    "id": "T-7h-gAJC8QF",
    "outputId": "e60c53e1-857b-49eb-dbda-230baa759ed7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 S1        S2        S3        S4\n",
      "all        0.000000  0.000000  0.000000  2.386294\n",
      "first      2.386294  0.000000  0.000000  0.000000\n",
      "forth      0.000000  0.000000  0.000000  2.386294\n",
      "here       0.000000  0.000000  2.386294  0.000000\n",
      "is         1.287682  1.287682  1.287682  0.000000\n",
      "of         0.000000  0.000000  0.000000  2.386294\n",
      "second     0.000000  2.386294  0.000000  0.000000\n",
      "sentence   1.287682  1.287682  1.287682  0.000000\n",
      "sentences  0.000000  0.000000  0.000000  2.386294\n",
      "the        1.693147  0.000000  1.693147  0.000000\n",
      "third      0.000000  0.000000  2.386294  0.000000\n",
      "this       1.693147  1.693147  0.000000  0.000000\n"
     ]
    }
   ],
   "source": [
    "# Sample text data with special characters and different formatting\n",
    "text = [\n",
    "    'this is the first     sentence!!',      # S1\n",
    "    'this is a second Sentence :)',          # S2\n",
    "    'the third sentence, is here ',          # S3\n",
    "    'forth of all sentences'                 # S4\n",
    "]\n",
    "# write code here\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cNhKc6MRCSdZ"
   },
   "source": [
    "##**Example: Online Discussions on Autos and Electronics**\n",
    "\n",
    "This example6 illustrates a classification task—to classify Internet discussion posts as either auto-related or electronics-related. One post looks like this:\n",
    "\n",
    "From: smith@logos.asd.sgi.com (Tom Smith) Subject: Ford Explorer 4WD - do I need performance axle?\n",
    "\n",
    "We’re considering getting a Ford Explorer XLT with 4WD and we have the following questions (All we would do is go skiing - no off-roading):\n",
    "\n",
    "1. With 4WD, do we need the “performance axle” - (limited slip axle). Its purpose is to allow the tires to act independently when the tires are on different terrain.\n",
    "\n",
    "2. Do we need the all-terrain tires (P235/75X15) or will the all-season (P225/70X15) be good enough for us at Lake Tahoe?\n",
    "\n",
    "Thanks,\n",
    "\n",
    "Tom\n",
    "\n",
    "–\n",
    "\n",
    "================================================\n",
    "\n",
    "Tom Smith Silicon Graphics smith@asd.sgi.com 2011 N. Shoreline Rd. MS 8U-815 415-962-0494 (fax) Mountain View, CA 94043\n",
    "\n",
    "================================================\n",
    "\n",
    "The posts are taken from Internet groups devoted to autos and electronics, so are pre-labeled. This one, clearly, is auto-related. A related organizational scenario might involve messages received by a medical office that must be classified as medical or non-medical (the messages in such a real scenario would probably have to be labeled by humans as part of the preprocessing).\n",
    "\n",
    "The posts are in the form a zipped file that contains two folders: auto posts and electronics posts, each contains a set of 1000 posts organized in small files. In the following, we describe the main steps from preprocessing to building a classification model on the data. Table 20.7 provides Python code for the text processing step for this example. We describe each step separately next.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uypQTgpdC8QF"
   },
   "source": [
    "## Table 20.7 Importing and labeling the records, preprocessing text, and producing concept matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZhTLzBSvCgfl"
   },
   "source": [
    "The ZipFile module in the standard library of Python is used to read the individual documents in the zipped datafile. We additionally create a label array that corresponds to the order of the documents—we will use “1” for autos and “0” for electronics. The assignment is based on the file path."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-26T14:23:16.719290Z",
     "iopub.status.busy": "2023-06-26T14:23:16.718459Z",
     "iopub.status.idle": "2023-06-26T14:23:21.831569Z",
     "shell.execute_reply": "2023-06-26T14:23:21.830637Z"
    },
    "id": "_3NUJ15LC8QF",
    "outputId": "3e03401d-e41e-4ab4-b333-91ec3edb6e03"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Import and label records\n",
    "corpus = []  # Initialize a list to store text data from each file\n",
    "label = []   # Initialize a list to store labels for each document (1 for autos, 0 for electronics)\n",
    "\n",
    "# Open the zipped data file containing documents for 'rec.autos' and 'comp.sys.electronics'\n",
    "with ZipFile(dmba.get_data_file('AutoAndElectronics.zip')) as rawData:\n",
    "    for info in rawData.infolist():  # Loop over each file in the archive\n",
    "        if info.is_dir():  # Skip directories, as we only want files\n",
    "            continue\n",
    "        # Assign a label of 1 if the file is from 'rec.autos', otherwise assign 0 (for 'comp.sys.electronics')\n",
    "        label.append(1 if 'rec.autos' in info.filename else 0)\n",
    "        # Read the file content and add it to the corpus list\n",
    "        corpus.append(rawData.read(info))\n",
    "\n",
    "# Step 2: Preprocessing (Tokenization, Stemming, and Stopword Removal)\n",
    "\n",
    "# Define a custom tokenizer class that includes stemming and stopword removal\n",
    "class LemmaTokenizer(object):\n",
    "    def __init__(self):\n",
    "        self.stemmer = EnglishStemmer()  # Initialize an English stemmer for reducing words to their base form\n",
    "        self.stopWords = set(ENGLISH_STOP_WORDS)  # Use a predefined set of English stopwords\n",
    "\n",
    "    def __call__(self, doc):\n",
    "        # Tokenize the document, apply stemming, and remove stopwords and non-alphabetic tokens\n",
    "        return [self.stemmer.stem(t) for t in word_tokenize(doc)\n",
    "                if t.isalpha() and t not in self.stopWords]\n",
    "\n",
    "# Create a CountVectorizer instance with the custom tokenizer and set encoding\n",
    "preprocessor = CountVectorizer(tokenizer=LemmaTokenizer(), encoding='latin1')\n",
    "\n",
    "# Apply the preprocessing steps (tokenization, stopword removal, stemming) and fit-transform the corpus\n",
    "preprocessedText = preprocessor.fit_transform(corpus)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JRMMaSS1FmM-"
   },
   "source": [
    "**Summary of Code Steps:**\n",
    "\n",
    "**1. Data Loading and Labeling:** Extracts and labels each document based on its source ('rec.autos' vs. 'comp.sys.electronics') and reads the document content into a list.\n",
    "\n",
    "**2. Preprocessing:**\n",
    "\n",
    "*   Tokenization: Breaks down documents into individual words.\n",
    "*   Stemming: Reduces words to their root form (e.g., \"cars\" becomes \"car\").\n",
    "*   Stopword Removal: Removes common English stopwords (like \"and\", \"the\") that are often not informative for classification.\n",
    "\n",
    "**3. Vectorization:** Converts the preprocessed text into a structured matrix format (bag-of-words), suitable for further analysis or modeling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-26T14:23:21.834831Z",
     "iopub.status.busy": "2023-06-26T14:23:21.834304Z",
     "iopub.status.idle": "2023-06-26T14:23:22.205310Z",
     "shell.execute_reply": "2023-06-26T14:23:22.204622Z"
    },
    "id": "3qoyHxxLC8QG"
   },
   "outputs": [],
   "source": [
    "# Step 3: TF-IDF and Latent Semantic Analysis (LSA)\n",
    "\n",
    "# Initialize a TF-IDF transformer to convert the count-based term matrix into TF-IDF (Term Frequency-Inverse Document Frequency)\n",
    "tfidfTransformer = TfidfTransformer()\n",
    "tfidf = tfidfTransformer.fit_transform(preprocessedText)\n",
    "# The TF-IDF representation gives higher weight to terms that are significant in a document but less common across the entire corpus.\n",
    "\n",
    "# Initialize TruncatedSVD for dimensionality reduction, targeting 20 latent concepts\n",
    "svd = TruncatedSVD(20)  # Extracts 20 latent \"concepts\" based on word co-occurrences\n",
    "\n",
    "# Initialize a normalizer to ensure that the LSA-transformed vectors have unit length\n",
    "normalizer = Normalizer(copy=False)\n",
    "\n",
    "# Create a pipeline that first applies SVD for dimensionality reduction, then normalizes the result\n",
    "lsa = make_pipeline(svd, normalizer)\n",
    "\n",
    "# Apply the LSA pipeline to the TF-IDF representation to get the transformed LSA feature matrix\n",
    "lsa_tfidf = lsa.fit_transform(tfidf)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "shTlk36eGHEA"
   },
   "source": [
    "Here's a breakdown with comments for this additional code:\n",
    "\n",
    "```python\n",
    "# Step 3: TF-IDF and Latent Semantic Analysis (LSA)\n",
    "\n",
    "# Initialize a TF-IDF transformer to convert the count-based term matrix into TF-IDF (Term Frequency-Inverse Document Frequency)\n",
    "tfidfTransformer = TfidfTransformer()\n",
    "tfidf = tfidfTransformer.fit_transform(preprocessedText)\n",
    "# The TF-IDF representation gives higher weight to terms that are significant in a document but less common across the entire corpus.\n",
    "\n",
    "# Initialize TruncatedSVD for dimensionality reduction, targeting 20 latent concepts\n",
    "svd = TruncatedSVD(20)  # Extracts 20 latent \"concepts\" based on word co-occurrences\n",
    "\n",
    "# Initialize a normalizer to ensure that the LSA-transformed vectors have unit length\n",
    "normalizer = Normalizer(copy=False)\n",
    "\n",
    "# Create a pipeline that first applies SVD for dimensionality reduction, then normalizes the result\n",
    "lsa = make_pipeline(svd, normalizer)\n",
    "\n",
    "# Apply the LSA pipeline to the TF-IDF representation to get the transformed LSA feature matrix\n",
    "lsa_tfidf = lsa.fit_transform(tfidf)\n",
    "```\n",
    "\n",
    "### Summary of Code Steps:\n",
    "1. **TF-IDF Transformation**:\n",
    "   - Converts the raw term counts to a TF-IDF matrix, which weighs terms based on their frequency within a document and inversely with their frequency across all documents. This transformation helps to reduce the influence of very common terms and highlights terms that are more informative within each document.\n",
    "\n",
    "2. **Latent Semantic Analysis (LSA)**:\n",
    "   - **Truncated SVD**: Reduces the dimensionality of the TF-IDF matrix by extracting the most significant components (in this case, 20), which represent underlying \"concepts\" in the data.\n",
    "   - **Normalization**: Ensures that each document vector has unit length, which can improve performance in downstream analyses by standardizing the feature magnitudes.\n",
    "   - The resulting `lsa_tfidf` matrix now represents each document in terms of the 20 extracted concepts, capturing underlying patterns in the data while reducing noise and dimensionality."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "L4By_TTMC8QG"
   },
   "source": [
    "## Table 20.8 Fitting a predictive model to the autos and electronics discussion data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-26T14:23:22.207918Z",
     "iopub.status.busy": "2023-06-26T14:23:22.207602Z",
     "iopub.status.idle": "2023-06-26T14:23:22.230604Z",
     "shell.execute_reply": "2023-06-26T14:23:22.230168Z"
    },
    "id": "bL9Ry1QnC8QG",
    "outputId": "3b79dee8-3945-4a37-caa8-7c8df8bcdd61"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix (Accuracy 0.9563)\n",
      "\n",
      "       Prediction\n",
      "Actual   0   1\n",
      "     0 388   9\n",
      "     1  26 377\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Split dataset into training and test sets\n",
    "# Use a 60/40 split, where 60% of the data is used for training and 40% for testing\n",
    "Xtrain, Xtest, ytrain, ytest = train_test_split(lsa_tfidf, label, test_size=0.4, random_state=42)\n",
    "# Here, `lsa_tfidf` represents the feature matrix with LSA-transformed data, and `label` contains the target labels.\n",
    "# `random_state=42` ensures reproducibility of the split.\n",
    "\n",
    "# Step 2: Train a logistic regression model on the training data\n",
    "logit_reg = LogisticRegression(solver='lbfgs')  # Initialize logistic regression with the 'lbfgs' solver (recommended for smaller datasets)\n",
    "logit_reg.fit(Xtrain, ytrain)  # Fit the model to the training data\n",
    "\n",
    "# Step 3: Evaluate model performance on the test set\n",
    "# Print the confusion matrix and accuracy to assess the model's performance\n",
    "classificationSummary(ytest, logit_reg.predict(Xtest))\n",
    "# `classificationSummary` provides a summary including the confusion matrix and accuracy,\n",
    "# helping to understand how well the model performs on the test data by showing true positives, false positives, etc.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6ovdl7_3GeaO"
   },
   "source": [
    "Here's a breakdown with comments for the code that fits a predictive model (logistic regression) to the autos and electronics discussion data, including splitting the dataset, training the model, and evaluating its performance:\n",
    "\n",
    "```python\n",
    "# Step 1: Split dataset into training and test sets\n",
    "# Use a 60/40 split, where 60% of the data is used for training and 40% for testing\n",
    "Xtrain, Xtest, ytrain, ytest = train_test_split(lsa_tfidf, label, test_size=0.4, random_state=42)\n",
    "# Here, `lsa_tfidf` represents the feature matrix with LSA-transformed data, and `label` contains the target labels.\n",
    "# `random_state=42` ensures reproducibility of the split.\n",
    "\n",
    "# Step 2: Train a logistic regression model on the training data\n",
    "logit_reg = LogisticRegression(solver='lbfgs')  # Initialize logistic regression with the 'lbfgs' solver (recommended for smaller datasets)\n",
    "logit_reg.fit(Xtrain, ytrain)  # Fit the model to the training data\n",
    "\n",
    "# Step 3: Evaluate model performance on the test set\n",
    "# Print the confusion matrix and accuracy to assess the model's performance\n",
    "classificationSummary(ytest, logit_reg.predict(Xtest))\n",
    "# `classificationSummary` provides a summary including the confusion matrix and accuracy,\n",
    "# helping to understand how well the model performs on the test data by showing true positives, false positives, etc.\n",
    "```\n",
    "\n",
    "### Summary of Code Steps:\n",
    "1. **Data Splitting**:\n",
    "   - The dataset is divided into a training set (60%) and a test set (40%) to train and evaluate the model separately. This split ensures that the model's performance can be tested on unseen data, giving an unbiased estimate of its accuracy.\n",
    "\n",
    "2. **Model Training**:\n",
    "   - A logistic regression model is initialized and trained on the LSA-transformed training data (`Xtrain`). Logistic regression is suitable for binary classification tasks like this one (autos vs. electronics discussion).\n",
    "\n",
    "3. **Model Evaluation**:\n",
    "   - The model’s predictions on the test set (`Xtest`) are compared with the actual labels (`ytest`) using `classificationSummary`, which prints the confusion matrix and accuracy. The confusion matrix helps in analyzing true positives, true negatives, false positives, and false negatives, while accuracy provides an overall measure of performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-26T14:23:22.233931Z",
     "iopub.status.busy": "2023-06-26T14:23:22.233420Z",
     "iopub.status.idle": "2023-06-26T14:23:22.745724Z",
     "shell.execute_reply": "2023-06-26T14:23:22.744670Z"
    },
    "id": "vcRw2ThEC8QG",
    "outputId": "69003b37-1e82-4192-97cd-3e4b143b99b4"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Create a DataFrame with predicted probabilities and actual labels\n",
    "df = pd.DataFrame(data={\n",
    "    'prob': [p[1] for p in logit_reg.predict_proba(Xtest)],  # Extract the probability of the positive class\n",
    "    'actual': ytest  # Actual test labels\n",
    "})\n",
    "# Sort the DataFrame by predicted probability in descending order\n",
    "df = df.sort_values(by=['prob'], ascending=False).reset_index(drop=True)\n",
    "\n",
    "# Plot the lift chart using the sorted DataFrame\n",
    "ax = liftChart(df.actual, labelBars=False)  # `labelBars=False` suppresses labels on the bars in the chart\n",
    "\n",
    "# Improve layout and display the plot\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9hLb2RVxGruL"
   },
   "source": [
    "Here’s an explanation and breakdown of this code, which generates a lift chart for evaluating the performance of the logistic regression model:\n",
    "\n",
    "```python\n",
    "# Create a DataFrame with predicted probabilities and actual labels\n",
    "df = pd.DataFrame(data={\n",
    "    'prob': [p[1] for p in logit_reg.predict_proba(Xtest)],  # Extract the probability of the positive class\n",
    "    'actual': ytest  # Actual test labels\n",
    "})\n",
    "# Sort the DataFrame by predicted probability in descending order\n",
    "df = df.sort_values(by=['prob'], ascending=False).reset_index(drop=True)\n",
    "\n",
    "# Plot the lift chart using the sorted DataFrame\n",
    "ax = liftChart(df.actual, labelBars=False)  # `labelBars=False` suppresses labels on the bars in the chart\n",
    "\n",
    "# Improve layout and display the plot\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "```\n",
    "\n",
    "### Explanation of Each Step:\n",
    "\n",
    "1. **Data Preparation**:\n",
    "   - `df = pd.DataFrame(data={'prob': [p[1] for p in logit_reg.predict_proba(Xtest)], 'actual': ytest})`:\n",
    "     - The `predict_proba` method of the logistic regression model (`logit_reg`) returns the probabilities for each class (in this case, `[p[1] for p in logit_reg.predict_proba(Xtest)]` extracts the probability of the positive class).\n",
    "     - This probability and the actual labels from `ytest` are stored in a new DataFrame, `df`, with columns `'prob'` and `'actual'`.\n",
    "\n",
    "2. **Sorting**:\n",
    "   - `df = df.sort_values(by=['prob'], ascending=False).reset_index(drop=True)`:\n",
    "     - The DataFrame `df` is sorted in descending order based on the predicted probability (`prob`), so that the instances with the highest predicted probability are at the top.\n",
    "     - The index is reset after sorting to maintain sequential indexing.\n",
    "\n",
    "3. **Lift Chart**:\n",
    "   - `ax = liftChart(df.actual, labelBars=False)`:\n",
    "     - The `liftChart` function (assuming it’s defined or imported from a library) plots the lift chart, which visualizes the model’s ability to capture the positive class effectively. It shows how well the model’s predictions perform compared to a random selection.\n",
    "\n",
    "4. **Displaying the Plot**:\n",
    "   - `plt.tight_layout()` ensures there is appropriate spacing in the layout.\n",
    "   - `plt.show()` displays the final lift chart.\n",
    "\n",
    "### Purpose of a Lift Chart:\n",
    "A lift chart helps assess the model’s effectiveness by showing how much better the model performs in identifying positive cases than a random model. This is particularly useful in cases where identifying a certain class (e.g., high-risk customers) is critical. The chart ranks instances by their predicted probability and provides insights into the model’s effectiveness at various probability thresholds."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
